#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Code Review Dashboard - Interface interativa para anÃ¡lise de cÃ³digo
"""

import os
import json
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from .analysis_engine import CodeAnalysisEngine, IssueType, Severity


class CodeReviewDashboard:
    """Dashboard interativo para code review"""
    
    def __init__(self, analysis_engine: CodeAnalysisEngine, ui_components):
        self.engine = analysis_engine
        self.ui = ui_components
        self.history_file = ".code_review_history.json"
        self.load_history()
        
    def show_main_dashboard(self):
        """Mostra dashboard principal de code review"""
        while True:
            self.ui.clear_screen()
            self.ui.header("ğŸ” CODE REVIEW DASHBOARD", "AnÃ¡lise Inteligente de CÃ³digo Python")
            
            # EstatÃ­sticas rÃ¡pidas
            self._display_quick_stats()
            
            print("\nğŸ› ï¸ ANÃLISE DE CÃ“DIGO:")
            print("1. ğŸ“ Analisar CÃ³digo Manual")
            print("2. ğŸ“ Analisar Arquivo Python")
            print("3. ğŸ§ª ExercÃ­cio com Code Review")
            print("4. ğŸ”„ Comparar VersÃµes")
            
            print("\nğŸ“Š RELATÃ“RIOS:")
            print("5. ğŸ“ˆ HistÃ³rico de Qualidade")
            print("6. ğŸ¯ AnÃ¡lise de Melhorias")
            print("7. ğŸ“‹ RelatÃ³rio Detalhado")
            print("8. ğŸ† Ranking de CÃ³digos")
            
            print("\nâš™ï¸ CONFIGURAÃ‡Ã•ES:")
            print("9. ğŸšï¸ Configurar AnÃ¡lise")
            print("10. ğŸ—‘ï¸ Limpar HistÃ³rico")
            
            print("\n0. ğŸ”™ Voltar")
            
            choice = input("\nğŸ‘‰ Escolha uma opÃ§Ã£o: ").strip()
            
            if choice == "0":
                break
            elif choice == "1":
                self._analyze_manual_code()
            elif choice == "2":
                self._analyze_file_code()
            elif choice == "3":
                self._exercise_with_review()
            elif choice == "4":
                self._compare_code_versions()
            elif choice == "5":
                self._show_quality_history()
            elif choice == "6":
                self._show_improvement_analysis()
            elif choice == "7":
                self._show_detailed_report()
            elif choice == "8":
                self._show_code_ranking()
            elif choice == "9":
                self._configure_analysis()
            elif choice == "10":
                self._clear_history()
            else:
                self.ui.error("OpÃ§Ã£o invÃ¡lida!")
                self.ui.pause()
    
    def _display_quick_stats(self):
        """Exibe estatÃ­sticas rÃ¡pidas"""
        history = self.engine.get_analysis_history()
        
        if not history:
            print("\nğŸ“Š ESTATÃSTICAS: Nenhuma anÃ¡lise realizada ainda")
            return
        
        recent_analyses = history[-5:]
        avg_score = sum(a['quality_score']['overall_score'] for a in recent_analyses) / len(recent_analyses)
        total_issues = sum(len(a['issues']) for a in recent_analyses)
        
        print(f"\nğŸ“Š ESTATÃSTICAS RÃPIDAS:")
        print(f"  ğŸ“ˆ Score mÃ©dio (Ãºltimas 5): {avg_score:.1f}")
        print(f"  ğŸ› Total de issues: {total_issues}")
        print(f"  ğŸ” AnÃ¡lises realizadas: {len(history)}")
        
        if recent_analyses:
            best_score = max(a['quality_score']['overall_score'] for a in recent_analyses)
            print(f"  ğŸ† Melhor score: {best_score:.1f}")
    
    def _analyze_manual_code(self):
        """AnÃ¡lise de cÃ³digo inserido manualmente"""
        self.ui.clear_screen()
        self.ui.header("ğŸ“ ANÃLISE MANUAL DE CÃ“DIGO", "Cole seu cÃ³digo Python para anÃ¡lise")
        
        print("âœï¸ Digite ou cole seu cÃ³digo Python:")
        print("ğŸ’¡ Digite '###END###' em uma linha vazia para finalizar")
        print("-" * 50)
        
        code_lines = []
        while True:
            try:
                line = input()
                if line.strip() == "###END###":
                    break
                code_lines.append(line)
            except EOFError:
                break
        
        code = '\n'.join(code_lines)
        
        if not code.strip():
            self.ui.warning("Nenhum cÃ³digo fornecido!")
            self.ui.pause()
            return
        
        # AnÃ¡lise
        print("\nğŸ” Analisando cÃ³digo...")
        result = self.engine.analyze_code(code, "manual_input.py", "manual")
        
        # Salva no histÃ³rico
        self.save_analysis(result)
        
        # Mostra resultado
        self._display_analysis_result(result)
    
    def _analyze_file_code(self):
        """AnÃ¡lise de arquivo Python"""
        self.ui.clear_screen()
        self.ui.header("ğŸ“ ANÃLISE DE ARQUIVO", "Analisar arquivo Python existente")
        
        filename = input("\nğŸ“‚ Nome do arquivo Python (ou caminho): ").strip()
        
        if not filename:
            self.ui.warning("Nome do arquivo nÃ£o fornecido!")
            self.ui.pause()
            return
        
        # Verifica se arquivo existe
        if not os.path.exists(filename):
            self.ui.error(f"Arquivo '{filename}' nÃ£o encontrado!")
            self.ui.pause()
            return
        
        # LÃª arquivo
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                code = f.read()
        except Exception as e:
            self.ui.error(f"Erro ao ler arquivo: {str(e)}")
            self.ui.pause()
            return
        
        # AnÃ¡lise
        print(f"\nğŸ” Analisando '{filename}'...")
        result = self.engine.analyze_code(code, filename, "file")
        
        # Salva no histÃ³rico
        self.save_analysis(result)
        
        # Mostra resultado
        self._display_analysis_result(result)
    
    def _exercise_with_review(self):
        """ExercÃ­cio com code review integrado"""
        self.ui.clear_screen()
        self.ui.header("ğŸ§ª EXERCÃCIO COM CODE REVIEW", "PrÃ¡tica guiada com anÃ¡lise")
        
        # ExercÃ­cios de exemplo
        exercises = [
            {
                "title": "Calculadora Simples",
                "description": "Crie uma funÃ§Ã£o que calcula operaÃ§Ãµes bÃ¡sicas",
                "template": '''def calculadora(a, b, operacao):
    # TODO: Implemente a calculadora
    pass

# Teste sua funÃ§Ã£o
resultado = calculadora(10, 5, "+")
print(resultado)''',
                "expected_features": ["function", "conditional", "return"]
            },
            {
                "title": "Validador de Email",
                "description": "Crie uma funÃ§Ã£o que valida endereÃ§os de email",
                "template": '''def validar_email(email):
    # TODO: Implemente validaÃ§Ã£o de email
    pass

# Teste sua funÃ§Ã£o
emails = ["test@example.com", "invalid-email", "user@domain.org"]
for email in emails:
    print(f"{email}: {validar_email(email)}")''',
                "expected_features": ["regex", "validation", "boolean_return"]
            },
            {
                "title": "Contador de Palavras",
                "description": "Conte palavras em um texto",
                "template": '''def contar_palavras(texto):
    # TODO: Implemente contador de palavras
    pass

# Teste sua funÃ§Ã£o
texto = "Python Ã© uma linguagem de programaÃ§Ã£o"
resultado = contar_palavras(texto)
print(resultado)''',
                "expected_features": ["string_processing", "dictionary", "loop"]
            }
        ]
        
        print("ğŸ¯ EXERCÃCIOS DISPONÃVEIS:")
        for i, ex in enumerate(exercises, 1):
            print(f"{i}. {ex['title']} - {ex['description']}")
        
        print("0. ğŸ”™ Voltar")
        
        try:
            choice = int(input("\nğŸ‘‰ Escolha um exercÃ­cio: "))
            if choice == 0:
                return
            if 1 <= choice <= len(exercises):
                exercise = exercises[choice - 1]
                self._run_guided_exercise(exercise)
            else:
                self.ui.error("ExercÃ­cio invÃ¡lido!")
                self.ui.pause()
        except ValueError:
            self.ui.error("Entrada invÃ¡lida!")
            self.ui.pause()
    
    def _run_guided_exercise(self, exercise: Dict[str, Any]):
        """Executa exercÃ­cio guiado"""
        self.ui.clear_screen()
        self.ui.header(f"ğŸ§ª {exercise['title']}", exercise['description'])
        
        print("ğŸ“‹ TEMPLATE INICIAL:")
        print("=" * 50)
        print(exercise['template'])
        print("=" * 50)
        
        print("\nâœï¸ Implemente sua soluÃ§Ã£o:")
        print("ğŸ’¡ Digite '###END###' para finalizar")
        
        code_lines = []
        while True:
            try:
                line = input()
                if line.strip() == "###END###":
                    break
                code_lines.append(line)
            except EOFError:
                break
        
        user_code = '\n'.join(code_lines)
        
        if not user_code.strip():
            user_code = exercise['template']
        
        # AnÃ¡lise com contexto do exercÃ­cio
        print("\nğŸ” Analisando sua soluÃ§Ã£o...")
        result = self.engine.analyze_code(user_code, f"exercise_{exercise['title']}.py", "exercise")
        
        # Salva no histÃ³rico
        self.save_analysis(result)
        
        # Mostra resultado com dicas especÃ­ficas do exercÃ­cio
        self._display_exercise_result(result, exercise)
    
    def _compare_code_versions(self):
        """Compara duas versÃµes do cÃ³digo"""
        self.ui.clear_screen()
        self.ui.header("ğŸ”„ COMPARAÃ‡ÃƒO DE VERSÃ•ES", "Compare melhorias no cÃ³digo")
        
        print("ğŸ“ VERSÃƒO 1 (cÃ³digo anterior):")
        print("Digite '###NEXT###' para passar Ã  prÃ³xima versÃ£o")
        
        code1_lines = []
        while True:
            try:
                line = input()
                if line.strip() == "###NEXT###":
                    break
                code1_lines.append(line)
            except EOFError:
                break
        
        print("\nğŸ“ VERSÃƒO 2 (cÃ³digo melhorado):")
        print("Digite '###END###' para finalizar")
        
        code2_lines = []
        while True:
            try:
                line = input()
                if line.strip() == "###END###":
                    break
                code2_lines.append(line)
            except EOFError:
                break
        
        code1 = '\n'.join(code1_lines)
        code2 = '\n'.join(code2_lines)
        
        if not code1.strip() or not code2.strip():
            self.ui.warning("Ambas as versÃµes devem ser fornecidas!")
            self.ui.pause()
            return
        
        # AnÃ¡lise de ambas as versÃµes
        print("\nğŸ” Analisando versÃµes...")
        result1 = self.engine.analyze_code(code1, "version_1.py", "comparison")
        result2 = self.engine.analyze_code(code2, "version_2.py", "comparison")
        
        # Mostra comparaÃ§Ã£o
        self._display_version_comparison(result1, result2)
    
    def _show_quality_history(self):
        """Mostra histÃ³rico de qualidade"""
        self.ui.clear_screen()
        self.ui.header("ğŸ“ˆ HISTÃ“RICO DE QUALIDADE", "EvoluÃ§Ã£o dos scores de cÃ³digo")
        
        history = self.engine.get_analysis_history()
        
        if not history:
            print("ğŸ“Š Nenhuma anÃ¡lise no histÃ³rico")
            self.ui.pause()
            return
        
        print("ğŸ“… HISTÃ“RICO DE ANÃLISES:")
        print("-" * 70)
        print(f"{'Data':<19} {'Arquivo':<25} {'Score':<8} {'Grade':<6} {'Issues'}")
        print("-" * 70)
        
        for analysis in history[-15:]:  # Ãšltimas 15
            timestamp = datetime.fromisoformat(analysis['timestamp'])
            filename = analysis['filename'][:24]
            score = analysis['quality_score']['overall_score']
            grade = analysis['quality_score']['grade']
            issues = len(analysis['issues'])
            
            print(f"{timestamp.strftime('%d/%m/%Y %H:%M'):<19} {filename:<25} {score:<8.1f} {grade:<6} {issues}")
        
        # EstatÃ­sticas
        scores = [a['quality_score']['overall_score'] for a in history]
        avg_score = sum(scores) / len(scores)
        best_score = max(scores)
        trend = "ğŸ“ˆ" if len(scores) > 1 and scores[-1] > scores[0] else "ğŸ“‰" if len(scores) > 1 and scores[-1] < scores[0] else "â¡ï¸"
        
        print("\nğŸ“Š ESTATÃSTICAS:")
        print(f"  ğŸ“ˆ Score mÃ©dio: {avg_score:.1f}")
        print(f"  ğŸ† Melhor score: {best_score:.1f}")
        print(f"  {trend} TendÃªncia: {'Melhorando' if trend == 'ğŸ“ˆ' else 'Piorando' if trend == 'ğŸ“‰' else 'EstÃ¡vel'}")
        
        self.ui.pause()
    
    def _show_improvement_analysis(self):
        """Mostra anÃ¡lise de melhorias"""
        self.ui.clear_screen()
        self.ui.header("ğŸ¯ ANÃLISE DE MELHORIAS", "PadrÃµes e sugestÃµes de crescimento")
        
        history = self.engine.get_analysis_history()
        
        if len(history) < 2:
            print("ğŸ“Š HistÃ³rico insuficiente para anÃ¡lise (mÃ­n. 2 anÃ¡lises)")
            self.ui.pause()
            return
        
        # AnÃ¡lise de tendÃªncias
        recent = history[-5:]
        
        # Issues mais comuns
        all_issues = []
        for analysis in recent:
            all_issues.extend(analysis['issues'])
        
        if not all_issues:
            print("ğŸ‰ Nenhum issue encontrado nas anÃ¡lises recentes!")
            self.ui.pause()
            return
        
        # Agrupa por tipo
        issue_types = {}
        for issue in all_issues:
            issue_type = issue['issue_type']
            if issue_type not in issue_types:
                issue_types[issue_type] = []
            issue_types[issue_type].append(issue)
        
        print("ğŸ” PADRÃ•ES DE ISSUES (Ãºltimas 5 anÃ¡lises):")
        print("-" * 50)
        
        for issue_type, issues in sorted(issue_types.items(), key=lambda x: len(x[1]), reverse=True):
            count = len(issues)
            print(f"  ğŸ“‹ {issue_type.upper()}: {count} ocorrÃªncias")
            
            # Mostra issues mais comuns deste tipo
            issue_messages = {}
            for issue in issues:
                msg = issue['message']
                issue_messages[msg] = issue_messages.get(msg, 0) + 1
            
            top_issues = sorted(issue_messages.items(), key=lambda x: x[1], reverse=True)[:3]
            for msg, freq in top_issues:
                print(f"    â€¢ {msg} ({freq}x)")
        
        # SugestÃµes de melhoria
        print("\nğŸ’¡ SUGESTÃ•ES DE MELHORIA:")
        most_common_type = max(issue_types.items(), key=lambda x: len(x[1]))
        
        suggestions = {
            'style': [
                "Use um formatter automÃ¡tico como black ou autopep8",
                "Configure seu editor para mostrar problemas de estilo",
                "Revise as convenÃ§Ãµes PEP 8"
            ],
            'logic': [
                "Pratique pensamento algorÃ­tmico",
                "Use debugging para entender o fluxo do cÃ³digo",
                "Revise padrÃµes de programaÃ§Ã£o Python"
            ],
            'security': [
                "CRÃTICO: Estude prÃ¡ticas de seguranÃ§a em Python",
                "Nunca use eval() ou exec() com dados nÃ£o confiÃ¡veis",
                "Use variÃ¡veis de ambiente para senhas"
            ],
            'maintainability': [
                "Refatore funÃ§Ãµes grandes em funÃ§Ãµes menores",
                "Use nomes descritivos para variÃ¡veis",
                "Adicione comentÃ¡rios explicativos"
            ],
            'documentation': [
                "Adicione docstrings Ã s suas funÃ§Ãµes",
                "Documente parÃ¢metros e valores de retorno",
                "Use comentÃ¡rios para lÃ³gica complexa"
            ]
        }
        
        if most_common_type[0] in suggestions:
            for suggestion in suggestions[most_common_type[0]]:
                print(f"  â€¢ {suggestion}")
        
        self.ui.pause()
    
    def _show_detailed_report(self):
        """Mostra relatÃ³rio detalhado da Ãºltima anÃ¡lise"""
        history = self.engine.get_analysis_history()
        
        if not history:
            self.ui.error("Nenhuma anÃ¡lise disponÃ­vel!")
            self.ui.pause()
            return
        
        latest = history[-1]
        self._display_analysis_result(latest, detailed=True)
    
    def _show_code_ranking(self):
        """Mostra ranking dos melhores cÃ³digos"""
        self.ui.clear_screen()
        self.ui.header("ğŸ† RANKING DE CÃ“DIGOS", "Seus melhores cÃ³digos por qualidade")
        
        history = self.engine.get_analysis_history()
        
        if not history:
            print("ğŸ“Š Nenhuma anÃ¡lise disponÃ­vel para ranking")
            self.ui.pause()
            return
        
        # Ordena por score
        ranked = sorted(history, key=lambda x: x['quality_score']['overall_score'], reverse=True)
        
        print("ğŸ† TOP 10 CÃ“DIGOS:")
        print("-" * 80)
        print(f"{'#':<3} {'Arquivo':<30} {'Score':<8} {'Grade':<6} {'Data':<12} {'Issues'}")
        print("-" * 80)
        
        for i, analysis in enumerate(ranked[:10], 1):
            timestamp = datetime.fromisoformat(analysis['timestamp'])
            filename = analysis['filename'][:29]
            score = analysis['quality_score']['overall_score']
            grade = analysis['quality_score']['grade']
            date = timestamp.strftime('%d/%m/%Y')
            issues = len(analysis['issues'])
            
            medal = "ğŸ¥‡" if i == 1 else "ğŸ¥ˆ" if i == 2 else "ğŸ¥‰" if i == 3 else f"{i:2d}"
            
            print(f"{medal:<3} {filename:<30} {score:<8.1f} {grade:<6} {date:<12} {issues}")
        
        self.ui.pause()
    
    def _configure_analysis(self):
        """Configura parÃ¢metros de anÃ¡lise"""
        self.ui.clear_screen()
        self.ui.header("âš™ï¸ CONFIGURAÃ‡ÃƒO DE ANÃLISE", "Personalize os critÃ©rios de review")
        
        print("ğŸšï¸ CONFIGURAÃ‡Ã•ES DISPONÃVEIS:")
        print("(Funcionalidade em desenvolvimento)")
        print("\nConfiguraÃ§Ã£o futura incluirÃ¡:")
        print("â€¢ Severidade mÃ­nima para alertas")
        print("â€¢ Tipos de anÃ¡lise habilitados")
        print("â€¢ Regras personalizadas")
        print("â€¢ Limites de mÃ©tricas")
        
        self.ui.pause()
    
    def _clear_history(self):
        """Limpa histÃ³rico de anÃ¡lises"""
        confirm = input("\nâš ï¸ Limpar TODO o histÃ³rico? (digite 'CONFIRMAR'): ")
        
        if confirm == "CONFIRMAR":
            self.engine.clear_history()
            if os.path.exists(self.history_file):
                os.remove(self.history_file)
            self.ui.success("âœ… HistÃ³rico limpo com sucesso!")
        else:
            self.ui.info("âŒ OperaÃ§Ã£o cancelada")
        
        self.ui.pause()
    
    def _display_analysis_result(self, result: Dict[str, Any], detailed: bool = False):
        """Exibe resultado da anÃ¡lise"""
        self.ui.clear_screen()
        self.ui.header("ğŸ” RESULTADO DA ANÃLISE", f"Arquivo: {result['filename']}")
        
        # Erro de sintaxe
        if not result['syntax_valid']:
            self.ui.error("âŒ ERRO DE SINTAXE:")
            print(f"  Linha {result['syntax_error']['line']}: {result['syntax_error']['message']}")
            self.ui.pause()
            return
        
        # Score geral
        score = result['quality_score']
        summary = result['summary']
        
        # Header com score
        score_color = "ğŸŸ¢" if score['overall_score'] >= 80 else "ğŸŸ¡" if score['overall_score'] >= 60 else "ğŸ”´"
        print(f"\n{score_color} SCORE GERAL: {score['overall_score']}/100 (Grade {score['grade']})")
        
        # Scores por categoria
        print(f"\nğŸ“Š SCORES POR CATEGORIA:")
        categories = [
            ("ğŸ¨ Estilo", score['style_score']),
            ("ğŸ§  LÃ³gica", score['logic_score']),
            ("ğŸ”’ SeguranÃ§a", score['security_score']),
            ("âš¡ Performance", score['performance_score']),
            ("ğŸ”§ Manutenibilidade", score['maintainability_score']),
            ("ğŸ“š DocumentaÃ§Ã£o", score['documentation_score'])
        ]
        
        for name, cat_score in categories:
            bar_length = int(cat_score / 10)
            bar = "â–ˆ" * bar_length + "â–‘" * (10 - bar_length)
            print(f"  {name:<18} {bar} {cat_score:5.1f}")
        
        # Issues
        issues = result['issues']
        if issues:
            print(f"\nğŸ› PROBLEMAS ENCONTRADOS ({len(issues)}):")
            
            # Agrupa por severidade
            by_severity = {}
            for issue in issues:
                sev = issue['severity']
                if sev not in by_severity:
                    by_severity[sev] = []
                by_severity[sev].append(issue)
            
            severity_icons = {
                'critical': 'ğŸš¨',
                'high': 'âš ï¸',
                'medium': 'ğŸ’›',
                'low': 'â„¹ï¸'
            }
            
            severity_order = ['critical', 'high', 'medium', 'low']
            
            for severity in severity_order:
                if severity in by_severity:
                    sev_issues = by_severity[severity]
                    print(f"\n  {severity_icons[severity]} {severity.upper()} ({len(sev_issues)}):")
                    
                    for issue in sev_issues[:5 if not detailed else None]:  # Limita se nÃ£o for detalhado
                        print(f"    ğŸ“ Linha {issue['line']}: {issue['message']}")
                        print(f"       ğŸ’¡ {issue['suggestion']}")
                    
                    if not detailed and len(sev_issues) > 5:
                        print(f"    ... e mais {len(sev_issues) - 5} issues")
        else:
            print(f"\nâœ… NENHUM PROBLEMA ENCONTRADO! CÃ³digo excelente!")
        
        # MÃ©tricas
        metrics = result['metrics']
        print(f"\nğŸ“ MÃ‰TRICAS DO CÃ“DIGO:")
        print(f"  ğŸ“„ Linhas de cÃ³digo: {metrics['lines_of_code']}")
        print(f"  ğŸ”§ FunÃ§Ãµes: {metrics['functions']}")
        print(f"  ğŸ“¦ Classes: {metrics['classes']}")
        print(f"  ğŸ§® Complexidade: {metrics['cyclomatic_complexity']}")
        print(f"  ğŸ“š DocumentaÃ§Ã£o: {metrics['documentation_coverage']:.1%}")
        
        # SugestÃµes
        suggestions = result['suggestions']
        if suggestions:
            print(f"\nğŸ’¡ PRINCIPAIS SUGESTÃ•ES:")
            for suggestion in suggestions:
                print(f"  â€¢ {suggestion}")
        
        # Tempo de anÃ¡lise
        print(f"\nâ±ï¸ AnÃ¡lise concluÃ­da em {result['analysis_time_ms']}ms")
        
        self.ui.pause()
    
    def _display_exercise_result(self, result: Dict[str, Any], exercise: Dict[str, Any]):
        """Exibe resultado especÃ­fico para exercÃ­cio"""
        self._display_analysis_result(result)
        
        # Feedback especÃ­fico do exercÃ­cio
        print(f"\nğŸ¯ FEEDBACK DO EXERCÃCIO '{exercise['title']}':")
        
        score = result['quality_score']['overall_score']
        if score >= 90:
            print("ğŸŒŸ EXCELENTE! Seu cÃ³digo estÃ¡ muito bem estruturado!")
        elif score >= 75:
            print("ğŸ‘ BOM TRABALHO! Apenas pequenos ajustes necessÃ¡rios.")
        elif score >= 60:
            print("ğŸ“š CONTINUAR PRATICANDO! HÃ¡ espaÃ§o para melhorias.")
        else:
            print("ğŸ¯ FOCO NA PRÃTICA! Revise os conceitos bÃ¡sicos.")
        
        # Verifica se implementou as funcionalidades esperadas
        expected = exercise.get('expected_features', [])
        if expected:
            print(f"\nâœ… FUNCIONALIDADES ESPERADAS:")
            for feature in expected:
                print(f"  â€¢ {feature}")
            print("  ğŸ’¡ Certifique-se de implementar todas as funcionalidades!")
        
        self.ui.pause()
    
    def _display_version_comparison(self, result1: Dict[str, Any], result2: Dict[str, Any]):
        """Exibe comparaÃ§Ã£o entre versÃµes"""
        self.ui.clear_screen()
        self.ui.header("ğŸ”„ COMPARAÃ‡ÃƒO DE VERSÃ•ES", "AnÃ¡lise de melhorias")
        
        score1 = result1['quality_score']['overall_score']
        score2 = result2['quality_score']['overall_score']
        
        diff = score2 - score1
        trend = "ğŸ“ˆ" if diff > 0 else "ğŸ“‰" if diff < 0 else "â¡ï¸"
        
        print(f"\n{trend} COMPARAÃ‡ÃƒO DE SCORES:")
        print(f"  ğŸ“ VersÃ£o 1: {score1:.1f} ({result1['quality_score']['grade']})")
        print(f"  ğŸ“ VersÃ£o 2: {score2:.1f} ({result2['quality_score']['grade']})")
        print(f"  ğŸ“Š DiferenÃ§a: {diff:+.1f} pontos")
        
        if diff > 0:
            print(f"\nğŸ‰ PARABÃ‰NS! Seu cÃ³digo melhorou {diff:.1f} pontos!")
        elif diff < 0:
            print(f"\nğŸ¤” Houve uma reduÃ§Ã£o de {abs(diff):.1f} pontos. Revise as mudanÃ§as.")
        else:
            print(f"\nâ¡ï¸ Score mantido. Continue refinando!")
        
        # ComparaÃ§Ã£o de issues
        issues1 = len(result1['issues'])
        issues2 = len(result2['issues'])
        
        print(f"\nğŸ› ISSUES:")
        print(f"  ğŸ“ VersÃ£o 1: {issues1} problemas")
        print(f"  ğŸ“ VersÃ£o 2: {issues2} problemas")
        
        if issues2 < issues1:
            print(f"  âœ… Corrigiu {issues1 - issues2} problemas!")
        elif issues2 > issues1:
            print(f"  âš ï¸ Introduziu {issues2 - issues1} novos problemas")
        
        # AnÃ¡lise por categoria
        print(f"\nğŸ“Š COMPARAÃ‡ÃƒO POR CATEGORIA:")
        categories = ['style_score', 'logic_score', 'security_score', 'performance_score', 'maintainability_score']
        
        for cat in categories:
            score1_cat = result1['quality_score'][cat]
            score2_cat = result2['quality_score'][cat]
            diff_cat = score2_cat - score1_cat
            
            cat_name = cat.replace('_score', '').title()
            trend_cat = "ğŸ“ˆ" if diff_cat > 0 else "ğŸ“‰" if diff_cat < 0 else "â¡ï¸"
            
            print(f"  {trend_cat} {cat_name}: {score1_cat:.1f} â†’ {score2_cat:.1f} ({diff_cat:+.1f})")
        
        self.ui.pause()
    
    def save_analysis(self, result: Dict[str, Any]):
        """Salva anÃ¡lise no histÃ³rico persistente"""
        try:
            history = self.load_history()
            history.append(result)
            
            # Limita histÃ³rico a 100 entradas
            if len(history) > 100:
                history = history[-100:]
            
            with open(self.history_file, 'w', encoding='utf-8') as f:
                json.dump(history, f, indent=2, ensure_ascii=False)
                
        except Exception as e:
            print(f"Erro ao salvar histÃ³rico: {e}")
    
    def load_history(self) -> List[Dict[str, Any]]:
        """Carrega histÃ³rico persistente"""
        try:
            if os.path.exists(self.history_file):
                with open(self.history_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
        except Exception:
            pass
        return []